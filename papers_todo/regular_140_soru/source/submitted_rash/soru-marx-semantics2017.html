<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="grammar/rash.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?>
<!DOCTYPE html>
<!-- 
SPARQL as a Foreign Language
by Tommaso Soru, Edgard Marx, Diego Moussallem, Gustavo Publio, André Valdestilhas, Diego Esteves, Ciro Baron Neto

This work is licensed under a Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).
You are free to:
* Share - copy and redistribute the material in any medium or format
* Adapt - remix, transform, and build upon the material
for any purpose, even commercially.

The licensor cannot revoke these freedoms as long as you follow the license terms.

Under the following terms:
* Attribution - You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.
-->
<html 
    xmlns="http://www.w3.org/1999/xhtml" 
    prefix="
        sd: https://w3id.org/scholarlydata/person/
        fabio: http://purl.org/spar/fabio/
        cito: http://purl.org/spar/cito/
        pro: http://purl.org/spar/pro/
        schema: http://schema.org/
        prism: http://prismstandard.org/namespaces/basic/2.0/
        frbr: http://purl.org/vocab/frbr/core#
        dcterms: http://purl.org/dc/terms/">
    <head>
        <!-- Visualisation requirements (mandatory for optimal reading) -->
        <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1"/>

        <link rel="stylesheet" href="css/bootstrap.min.css"/>
        <link rel="stylesheet" href="css/rash.css"/>

        <script src="js/jquery.min.js"> </script>
        <script src="js/bootstrap.min.js"> </script>
        <script src="js/rash.js"> </script>
		
        <!-- /END Visualisation requirements (mandatory for optimal reading) -->

        <!-- Metadata -->
        <title property="dcterms:title" typeof="fabio:ResearchPaper">SPARQL as a Foreign Language</title>

        <!-- Author's data (one or more) -->
        <meta about="sd:tommaso-soru" typeof="schema:Person" name="dc.creator" property="schema:name" content="Tommaso Soru"/>
        <meta about="sd:tommaso-soru" property="schema:email" content="tsoru@informatik.uni-leipzig.de"/>
        <link about="sd:tommaso-soru" property="schema:affiliation" href="https://w3id.org/scholarlydata/organisation/aksw"/>

        <meta about="sd:edgard-marx" typeof="schema:Person" name="dc.creator" property="schema:name" content="Edgard Marx"/>
        <meta about="sd:edgard-marx" property="schema:email" content="edgard.marx@htwk-leipzig.de"/>
        <link about="sd:edgard-marx" property="schema:affiliation" href="https://w3id.org/scholarlydata/organisation/leipzig-university-of-applied-sciences"/>

        <meta about="sd:diego-moussallem" typeof="schema:Person" name="dc.creator" property="schema:name" content="Diego Moussallem"/>
        <meta about="sd:diego-moussallem" property="schema:email" content="moussallem@informatik.uni-leipzig.de"/>
        <link about="sd:diego-moussallem" property="schema:affiliation" href="https://w3id.org/scholarlydata/organisation/aksw"/>

        <meta about="sd:gustavo-publio" typeof="schema:Person" name="dc.creator" property="schema:name" content="Gustavo Publio"/>
        <meta about="sd:gustavo-publio" property="schema:email" content="gustavo.publio@informatik.uni-leipzig.de"/>
        <link about="sd:gustavo-publio" property="schema:affiliation" href="https://w3id.org/scholarlydata/organisation/aksw"/>

        <meta about="sd:andre-valdestilhas" typeof="schema:Person" name="dc.creator" property="schema:name" content="Andr&eacute; Valdestilhas"/>
        <meta about="sd:andre-valdestilhas" property="schema:email" content="valdestilhas@informatik.uni-leipzig.de"/>
        <link about="sd:andre-valdestilhas" property="schema:affiliation" href="https://w3id.org/scholarlydata/organisation/aksw"/>

        <meta about="sd:diego-esteves" typeof="schema:Person" name="dc.creator" property="schema:name" content="Diego Esteves"/>
        <meta about="sd:diego-esteves" property="schema:email" content="esteves@cs.uni-bonn.de"/>
        <link about="sd:diego-esteves" property="schema:affiliation" href="https://w3id.org/scholarlydata/organisation/university-of-bonn"/>

        <meta about="sd:ciro-baron-neto" typeof="schema:Person" name="dc.creator" property="schema:name" content="Ciro Baron Neto"/>
        <meta about="sd:ciro-baron-neto" property="schema:email" content="cbaron@informatik.uni-leipzig.de"/>
        <link about="sd:ciro-baron-neto" property="schema:affiliation" href="https://w3id.org/scholarlydata/organisation/aksw"/>

        <!-- Affiliations -->
        <meta about="https://w3id.org/scholarlydata/organisation/aksw" property="schema:name" typeof="schema:Organization" content="Agile Knowledge Engineering and Semantic Web, Institute of Computer Science, University of Leipzig, Germany"/>
        <meta about="https://w3id.org/scholarlydata/organisation/leipzig-university-of-applied-sciences" property="schema:name" typeof="schema:Organization" content="Faculty of Computer Science Mathematics and Natural Sciences, Leipzig University of Applied Sciences, Germany"/>
        <meta about="https://w3id.org/scholarlydata/organisation/university-of-bonn" property="schema:name" typeof="schema:Organization" content="Smart Data Analytics, University of Bonn, Germany"/>

        <!-- Paper keywords (one or more) -->
        <meta property="prism:keyword" content="Question Answering"/>
        <meta property="prism:keyword" content="Machine Translation"/>
        <meta property="prism:keyword" content="Neural Networks"/>
        <meta property="prism:keyword" content="Deep Learning"/>
        <meta property="prism:keyword" content="SPARQL"/>
        <meta property="prism:keyword" content="Linked Data"/>
        <!-- /END Metadata -->
    </head>
    <body>
        <!-- Abstract -->
        <section role="doc-abstract">
            <h1>Abstract</h1>
            <p>
Recently, the Linked Data Cloud has achieved a size of more than 100 billion facts pertaining to a multitude of domains. 
However, accessing this information has been significantly challenging for lay users. 
Approaches to problems such as Question Answering on Linked Data and Link Discovery have notably played a role in increasing information access.
These approaches are often based on handcrafted and/or statistical models derived from data observation. 
Recently, Deep Learning architectures based on Neural Networks called seq2seq have shown to achieve the state-of-the-art results at translating sequences into sequences. 
In this direction, we propose Neural SPARQL Machines, end-to-end deep architectures to translate any natural language expression into sentences encoding SPARQL queries. 
Our preliminary results, restricted on selected DBpedia classes, show that Neural SPARQL Machines are a promising approach for Question Answering on Linked Data, as they can deal with known problems such as vocabulary mismatch and perform graph pattern composition.
			</p>
            <p><strong>Notes:</strong> Accepted at <a href="http://semantics.cc/">SEMANTiCS 2017</a> Posters &amp; Demos. The first two authors contributed equally.</p>
            <p><strong>This version:</strong> <a href="http://w3id.org/neural-sparql-machines/soru-marx-semantics2017.html">http://w3id.org/neural-sparql-machines/soru-marx-semantics2017.html</a></p>
            <p><strong>Last version:</strong> <a href="http://w3id.org/neural-sparql-machines/soru-marx-semantics2017.html">http://w3id.org/neural-sparql-machines/soru-marx-semantics2017.html</a></p>
        </section>

        <!-- Sections -->
        <section id="sec_introduction">
            <h1>Introduction</h1>
            <p>
Nowadays, the Web has been growing notably and producing an enormous amount of information every day. Meanwhile, the Linked Data Cloud has been structuring these billions of facts from the Web. 
However, this enormous volume of data complicates the access of a given desired information. 
Thus, becoming a hard task for lay users that are not familiar with formal query languages such as SPARQL<a href="#fn1"> </a> to search their interests. 
To this end, Question Answering (QA) systems provide users with friendly interfaces that hide the complexity of creating such queries. 
These interfaces enable the users to perform questions using their natural language.
Although the QA systems have shown great solutions for dealing with this huge amount of statements, QA still lacks good solutions for handling vocabulary mismatch and problems on graph patterns<a href="#fn2"> </a> <a href="#hoffner2016survey"> </a>. 
Vocabulary mismatch occurs when the user types different terms of the ones contained in a Knowledge Base (KB) for querying a given information. 
Several works tried to alleviate this problem, such as <a href="#shekarpour2017rquery"> </a><a href="#Dubey2016"> </a><a href="#lukovnikov2017www"> </a>.  
			</p>
            <p>
Recent advances in Deep Learning applied to Natural Language Processing tasks have shown a promising possibility of parsing, understanding, and translating language sentences. 
In addition, with the growth of embeddings-based techniques, the Machine Translation (MT) community brought back the application of Neural Networks (NN) within MT systems.
The first Neural Machine Translation (NMT) approach was proposed by Kalchbrenner and Blunsom <a href="#kalchbrenner2013recurrent"> </a>. 
This approach implemented Deep Neural Networks to build end-to-end encoder–decoder models using embeddings, i.e. vector spaces, within the language models. 
NMT recently overcame the Phrased-based Machine Translation approach on the BLEU <a href="#papineni2002bleu"> </a> score, thus rising the interest on NN applied to translation problems <a href="#wu2016google"> </a>. 
In NMT, pairs of sequences are given as input to a NN model, which is left to learn the translation model.
Based on this formal description, we have an insight to translate from Natural Language (NL) to SPARQL by using NMT models in order to address the aforementioned QA gaps. 
Therefore, we propose Neural SPARQL Machine, an end-to-end learning model to translate any NL expression into a sequence of tokens which can reconstruct a SPARQL grammar. 
Our preliminary results show that Neural SPARQL Machines can perform compositions of new graph patterns which never occurred in the training set. 
In addition, when evaluated on BLEU, our Neural SPARQL Machine achieved an accuracy of 0.8. 
			</p>

        </section>

        <section id="sec_related_work">
            <h1>Related work</h1>
            <p>
QA approaches commonly convert NL questions into formal SPARQL queries in order to retrieve the answer from a <i>triple store</i> <a href="#unger2012template"> </a><a href="#SINA_WebSemantic"> </a><a href="#Dubey2016"> </a>.
Approaches are designed to answer only facts, fact-based QA <a href="#lukovnikov2017www"> </a>, multiple facts Basic Graph Patterns <a href="#SINA_WebSemantic"> </a><a href="#Zhang2016"> </a> or more complex queries <a href="#unger2012template"> </a><a href="#Dubey2016"> </a>.
Most commonly, QA systems are based on a conjunction of previously-made rules derived from data analyses and observation <a href="#SINA_WebSemantic"> </a><a href="#Zhang2016"> </a><a href="#Dubey2016"> </a> or learned based on positive and negative samples <a href="#unger2012template"> </a><a href="#lukovnikov2017www"> </a><a href="#yih2015"> </a>.
Therefore, the strategy used to generate the SPARQL graph patterns and consequently the SPARQL query varies significantly.
			</p>
            <p>
<i>TBSL</i> <a href="#unger2012template"> </a> is a template-based QA system that relies on SPARQL query templates to generate queries with structures that resemble the semantic structure of the original query.
The template is detected based on learned positive and negative examples.
The predicate and entity detection are performed by applying a <i>TF-IDF</i> function implemented in the <i>Lucene Apache Framework</i><a href="#fn4"> </a>. 
			</p>
            <p>
<i>SINA</i> <a href="#SINA_WebSemantic"> </a> is a QA system that explores the knowledge base to formulate the SPARQL query by applying Jaccard similarity to evaluates possible resource matching candidates and Hidden Markov Models to choose the right answer among them. 
In this conversion, queries are identified (query-type) based on pre-defined syntactic rules and resources are linked using DBpedia Spotlight <a href="#isem2013daiber"> </a>. 
Recently, <a href="#lukovnikov2017www"> </a> proposed a fact retrieval approach based on a Recurrent Neural Network (RNN) encoding subject labels at character level and predicates and datatype values at both character and word levels.
			</p>
		</section>

        <section id="sec_approach">
            <h1>Neural SPARQL Machine</h1>
            <p>
We then propose a new paradigm for answering NL questions. Instead of using statistical and handcraft models, we shift the complete task of SPARQL query generation to Neural Simbolic Machines (NSM) which are a specific class of <i>seq2seq</i> models.
NSMs aim at translating NL to a sequence of tokens defining a program by using a key-variable memory and reinforcement learning with weak supervision <a href="#liang2016neural"> </a>. 
However, our approach for QA differs slightly from the one proposed by <a href="#liang2016neural"> </a>, as the authors 1) train a NN on query templates to avoid over-fitting and 2) use reinforcement learning to optimize the task reward of the problem.
We instead use an end-to-end approach that translates an entire NL expression into a final query (see an example in <a href="#fig_example"> </a>).
			</p>
			<figure id="fig_example">
				<p>
					<img src="img/seq2seq.svg" width="75%" alt="Example of seq2seq."/>
				</p>
				<figcaption>LSTM architecture for sequence-to-sequence learning of a machine translator from natural language to SPARQL. The left side is the <i>encoder</i>, whereas the right side shows the <i>decoder</i>. The question <i>"Where is Hill 60 located in?"</i> is translated into the sequence of tokens: <code>select var_a where br_open dbr_Hill_60_(Ypres) dbo_location var_a br_close</code>.</figcaption>
			</figure>
	        <section id="sec_architecture">
	            <h2>Architecture</h2>
	            <p>
The architecture of our Neural SPARQL Machine relies on three main components: a <i>generator</i>, a <i>learner</i>, and an <i>interpreter</i>. 
The generator takes query templates as input and creates the training dataset which will be forwarded to the learner. 
A <i>query template</i> is an alignment between a natural language query and its respective SPARQL query, with entities replaced by placeholders. 
The learner takes natural language as input and generates a sequence which encodes a SPARQL query.
The final structure is then reconstructed by the interpreter.
				</p>
				<figure id="fig_arch">
					<p>
						<img src="img/neural-sparql-machines.svg" width="50%" alt="Neural SPARQL Machines."/>
					</p>
					<figcaption>Architecture of a Neural SPARQL Machine at training phase (left) and prediction phase (right).</figcaption>
				</figure>
	            <p>
An overview of the architecture can be seen in <a href="#fig_arch"> </a>, where we highlighted the differences between the training and the prediction phases of the machine-learning workflow.
Below are a few examples of query templates:
<pre>
where is &lt;A&gt; located in?
  ->  select ?a where { &lt;A&gt; dbo:location ?a }
what are the &lt;A&gt; northernmost &lt;B&gt;?
  ->  select ?a where { ?a rdf:type &lt;B&gt; . ?a geo:lat ?b }
      order by desc(?b) limit &lt;A&gt;   
was &lt;A&gt; finished before &lt;B&gt;?
  ->  ask where { &lt;A&gt; dbp:complete ?a . FILTER(?a <= &lt;B&gt;) }
</pre>
				</p>
			</section>
	        <section id="sec_sparql_modeling">
	            <h2>SPARQL Modeling</h2>
	            <p>
We focus on learning a model which is suited for a specific KB and therefore we do not expect to reuse a learned model for QA on other KBs.
The rationale behind this viewpoint complies the concept of the Semantic Web, which allows users to define their own classes, instances, and properties without a need of a global design.
For instance, a model trained on DBpedia may learn that <i>"is located in"</i> translates to <code>dbo:country</code>.
However, property <code>dbo:country</code> might not have any sense in another dataset, where another property is used instead. For example, the property <code>http://schema.org/Country</code> used in the Schema.org ontology.
				</p>
	            <p>
Each of the SPARQL operators (e.g., <code>SELECT</code>, <code>FILTER</code>, or <code>ORDER BY</code>) in a query is encoded using a certain number of tokens.
We decided to encode brackets, wildcards, and dots, since they are also used inside the query.
URIs are shortened using prefixes and column characters replaced with underscores to keep each URI as a single token.
				</p>
			</section>
	        <section id="sec_the_learner">
	            <h2>The Learner</h2>
	            <p>
Our learner features a <i>seq2seq</i> model with Long Short-Term Memory (LSTM).
The seq2seq model takes a sequence of word embeddings
<!-- (your LaTeX) $X = \{x_1, x_2, ..., x_M\}$ -->
<math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
	<mi>X</mi>
	<mo>=</mo>
	<mrow>
		<mo form="prefix">{</mo>
		<msub>
			<mi>x</mi>
			<mn>1</mn>
		</msub>
		<mo>,</mo>
		<msub>
			<mi>x</mi>
			<mn>2</mn>
		</msub>
		<mo>,</mo>
		<mo>...</mo>
		<mo>,</mo>
		<msub>
			<mi>x</mi>
			<mi>M</mi>
		</msub>
		<mo form="postfix">}</mo>
	</mrow>
</mrow>
</math>
as input and outputs a translated sequence
<!-- (your LaTeX) $Y = \{ y_1, y_2, ..., y_N \}.$ -->
<math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
	<mi>Y</mi>
	<mo>=</mo>
	<mrow>
		<mo form="prefix">{</mo>
		<msub>
			<mi>y</mi>
			<mn>1</mn>
		</msub>
		<mo>,</mo>
		<msub>
			<mi>y</mi>
			<mn>2</mn>
		</msub>
		<mo>,</mo>
		<mo>...</mo>
		<mo>,</mo>
		<msub>
			<mi>y</mi>
			<mi>N</mi>
		</msub>
		<mo form="postfix">}</mo>
	</mrow>
	<mo>.</mo>
</mrow>
</math>
Aim of the model is to maximize the generation probability of <i>Y</i> conditioned on <i>X</i>, i.e.
<!-- (your LaTeX) $p(y_1, ..., y_N | x_1, ..., x_M).$ -->
<math xmlns="http://www.w3.org/1998/Math/MathML">
<mrow>
	<mi>p</mi>
	<mrow>
		<mo form="prefix">(</mo>
		<msub>
			<mi>y</mi>
			<mn>1</mn>
		</msub>
		<mo>,</mo>
		<mo>...</mo>
		<mo>,</mo>
		<msub>
			<mi>y</mi>
			<mi>N</mi>
		</msub>
		<mo>|</mo>
		<msub>
			<mi>x</mi>
			<mn>1</mn>
		</msub>
		<mo>,</mo>
		<mo>...</mo>
		<mo>,</mo>
		<msub>
			<mi>x</mi>
			<mi>M</mi>
		</msub>
		<mo form="postfix">)</mo>
	</mrow>
	<mo>.</mo>
</mrow>
</math>
As seen in <a href="#fig_example"> </a>, the model is composed by a encoder and a decoder.
Using a RNN, an encoder <i>q</i> reads the input words into a vector:
<figure id="formula_c">
	<p>
		<!-- (your LaTeX) $c=q(h_1, ..., h_T).$ -->
		<math xmlns="http://www.w3.org/1998/Math/MathML">
		<mrow>
			<mi>c</mi>
			<mo>=</mo>
			<mi>q</mi>
			<mrow>
				<mo form="prefix">(</mo>
				<msub>
					<mi>h</mi>
					<mn>1</mn>
				</msub>
				<mo>,</mo>
				<mo>...</mo>
				<mo>,</mo>
				<msub>
					<mi>h</mi>
					<mi>T</mi>
				</msub>
				<mo form="postfix">)</mo>
			</mrow>
			<mo>.</mo>
		</mrow>
		</math>
	</p>
</figure>
The hidden states at a time <i>t</i> of the NN are computed as follows:
<figure id="formula_ht">
	<p>
		<!-- (your LaTeX) $h_t = f(x_t, h_{t-1})$ -->
		<math xmlns="http://www.w3.org/1998/Math/MathML">
		<mrow>
			<msub>
				<mi>h</mi>
				<mi>t</mi>
			</msub>
			<mo>=</mo>
			<mi>f</mi>
			<mrow>
				<mo form="prefix">(</mo>
				<msub>
					<mi>x</mi>
					<mi>t</mi>
				</msub>
				<mo>,</mo>
				<msub>
					<mi>h</mi>
					<mrow>
						<mi>t</mi>
						<mo>-</mo>
						<mn>1</mn>
					</mrow>
				</msub>
				<mo form="postfix">)</mo>
			</mrow>
		</mrow>
		</math>
	</p>
</figure>
where </i>f</i> is a non-linear transformation performed by a LSTM using sigmoid function.
The same workflow applies specularly for the decoder, after which the output sequence <i>Y</i> is outputted.
				</p>
	            <p>
The plain version of our learner learns word embeddings from the input corpus.
However, as previously proposed in <a href="#liang2016neural"> </a>, word embeddings can also be imported from a pre-trained corpus of Word2Vec <a href="#mikolov2013distributed"> </a> vectors to help tackling known QA problems such as vocabulary mismatch.
				</p>
			</section>
		</section>

        <section id="sec_preliminary_results">
            <h1>Preliminary results</h1>
            <p>
			</p>
	        <section id="sec_data_generation">
	            <h2>Data generation</h2>
	            <p>
We use DBpedia as a use-case for our preliminary test, considering the extension of one class as working domain, i.e. <code>dbo:Monument</code>.
The class extension contains 35,095 triples and 625 instances.
To generate the training data, we manually annotated 38 query templates involving one or two entities per query.
For each of these templates, we fetched from DBpedia a given number of examples that satisfied the corresponding SPARQL graph patterns.
For instance, for question <i>"where is &lt;A&gt; located in?"</i>, we fetched only instances having a <code>dbo:location</code> property.
These instances were randomly extracted and added to the working dataset.
We split the working dataset into three parts: training, validation, and test dataset.
We fixed the size of the validation and test sets to 100 examples.
				</p>
			</section>
	        <section id="sec_experiments">
	            <h2>Experiments</h2>
	            <p>
We carried out an evaluation of the BLEU accuracy <a href="#papineni2002bleu"> </a> for the NMT model. 
BLEU uses a modified precision metric for comparing the MT output with the reference translation, in our case, SPARQL queries.
As <a href="#tab_exp"> </a> shows, the first setting expects to have an average of 13.35 examples per instance, meaning that the model has &mdash; on average &mdash; around 13-14 times to learn that certain labels translate into a certain URI.
				</p>
				<figure id="tab_exp">
				    <table>
				        <tr>
				            <th>Dataset</th>
				            <th>Examples per template</th>
				            <th>Training size</th>
				            <th>Average examples per instance</th>
				        </tr>
				        <tr>
				            <td><p><code>dbo:Monument</code></p></td>
				            <td><p>300</p></td>
				            <td><p>8,344</p></td>
				            <td><p>13.35</p></td>
				        </tr>
				        <tr>
				            <td><p><code>dbo:Monument</code></p></td>
				            <td><p>600</p></td>
				            <td><p>14,588</p></td>
				            <td><p>23.34</p></td>
				        </tr>
				    </table>
				    <figcaption>Information about the experiments carried out on the datasets.</figcaption>
				</figure>
	            <p>
The experiments were carried out on a 64-core CPU-only Ubuntu machine with 512 GB RAM.<a href="#fn3"> </a>
We adopted the implementation of seq2seq in TensorFlow <a href="#abadi2016tensorflow"> </a> with embeddings of 128 dimensions, 2 hidden layers, and a dropout value of 0.2.
We tested our NMT model at three different times, i.e. at the 12,000th, 36,000th, and 120,000th iteration.
				</p>
				<figure id="tab_results">
				    <table>
				        <tr>
				            <th>Dataset</th>
				            <th>Examples per template</th>
				            <th>BLEU 12k epochs</th>
				            <th>BLEU 36k epochs</th>
				            <th>BLEU 120k epochs</th>
				        </tr>
				        <tr>
				            <td><p><code>dbo:Monument</code></p></td>
				            <td><p>300</p></td>
				            <td><p>0.753 (0:18:24)</p></td>
				            <td><p>0.767 (0:55:28)</p></td>
				            <td><p>0.765 (3:05:43)</p></td>
				        </tr>
				        <tr>
				            <td><p><code>dbo:Monument</code></p></td>
				            <td><p>600</p></td>
				            <td><p>0.801 (0:19:10)</p></td>
				            <td><p>0.803 (0:58:09)</p></td>
				            <td><p>0.803 (3:11:44)</p></td>
				        </tr>
				    </table>
				    <figcaption>BLEU accuracy after a given training time (expressed as H:MM:SS) for different number of examples per query template.</figcaption>
				</figure>
	            <p>
The complete evaluation results are shown in <a href="#tab_results"> </a>.
One of the main drawbacks of deep-learning architectures is the computational complexity and training time.
As can be seen, the number of examples per template does not seem to affect the training time as much as the number of iterations.
However, the BLEU accuracy achieved with <i>n=600</i> saw an increase of <i>+5%</i>, which can lead to think that more examples might further improve the translation.
As <a href="#fig_bleu"> </a> shows, this value stabilizes after 10,000 epochs, which correspond to &#126;16 minutes of runtime.
Minimizing the number of examples per instance, we estimate to learn a model on a large dataset as DBpedia in less than 20 days; GPU architectures can bring this value down to 5 days.
Since the BLEU accuracy gives only an estimation of how the alignments are performed between the languages, we plan to use standard measures such as F-score to evaluate the final prediction of SPARQL queries.
				</p>
				<figure id="fig_bleu">
					<p>
						<img src="img/bleu.svg" width="50%" alt="BLEU accuracy over epochs."/>
					</p>
					<figcaption>BLEU accuracy on the test set for the translations on dataset <code>dbo:Monument</code> with 600 examples per query template.</figcaption>
				</figure>
	            <p>
To test compositionality, we inputted complex questions to our Neural SPARQL Machine.
An example is <i>"where are the 3 northernmost monuments located in?"</i>.
Although the model never saw such a question before, it was able to output a sequence which includes the features of two learned templates: <code>select var_a where br_open var_a rdf_type dbo_Monument sep_dot var_a geo_lat var_b sep_dot var_a dbo_location var_b br_close order_by desc var_b limit 3</code>.
Still, the interpreter will need to replace variables <code>var_a</code> or <code>var_b</code> with a new variable <code>var_c</code> to fetch the desired information.
This is an open challenge for our model.
				</p>
			</section>
		</section>

        <section id="sec_conclusion_and_future_work">
            <h1>Conclusion and future work</h1>
            <p>
In this work, we presented a neural approach for the translation of NL questions in SPARQL queries using a NN model.
Preliminary results show an average BLEU accuracy of &#126;0.8 when trained on QA template model pairs, even when answering such questions requires the formulation of more complex queries. 
The current model is vocabulary-dependent and needs to be trained on samples derived from manually-created QA template pairs. 
We plan to address current limitations by investigating how to generate domain-independent templates and minimize the burden on the end user.
			</p>
		</section>

        <!-- References -->
        <section role="doc-bibliography">
            <h1>References</h1>
            <ul about="" rel="cito:cites">
			<li id="mikolov2013distributed" about="mikolov2013distributed-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="mikolov2013distributed-work" role="doc-biblioentry">
				<p about="mikolov2013distributed-expression" property="dcterms:bibliographicCitation">Mikolov T., Sutskever I., Chen K., Corrado G., Dean J. (2013). Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems.</p>
			</li>
			<li id="hoffner2016survey" about="hoffner2016survey-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="hoffner2016survey-work" role="doc-biblioentry">
				<p about="hoffner2016survey-expression" property="dcterms:bibliographicCitation">Hoffner K., Walter S., Marx E., Usbeck R., Lehmann J., Ngonga Ngomo A. (2016). Survey on challenges of Question Answering in the Semantic Web. Semantic Web.</p>
			</li>
			<li id="shekarpour2017rquery" about="shekarpour2017rquery-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="shekarpour2017rquery-work" role="doc-biblioentry">
				<p about="shekarpour2017rquery-expression" property="dcterms:bibliographicCitation">Shekarpour S., Marx E., Auer S., Sheth A. (2017). RQUERY: Rewriting Natural Language Queries on Knowledge Graphs to Alleviate the Vocabulary Mismatch Problem. AAAI.</p>
			</li>
			<li id="liang2016neural" about="liang2016neural-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="liang2016neural-work" role="doc-biblioentry">
				<p about="liang2016neural-expression" property="dcterms:bibliographicCitation">Liang C., Berant J., Le Q., Forbus K., Lao N. (2016). Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. arXiv preprint arXiv:1611.00020.</p>
			</li>
			<li id="Dubey2016" about="Dubey2016-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="Dubey2016-work" role="doc-biblioentry">
				<p about="Dubey2016-expression" property="dcterms:bibliographicCitation">Dubey M., Dasgupta S., Sharma A., Hoffner K., Lehmann J. (2016). AskNow: A Framework for Natural Language Query Formalization in SPARQL. The Semantic Web. Latest Advances and New Domains: 13th International Conference, ESWC 2016, Heraklion, Crete, Greece, May 29 -- June 2, 2016, Proceedings.</p>
			</li>
			<li id="Zhang2016" about="Zhang2016-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="Zhang2016-work" role="doc-biblioentry">
				<p about="Zhang2016-expression" property="dcterms:bibliographicCitation">Zhang Y., He S., Liu K., Zhao J. (2016). A Joint Model for Question Answering over Multiple Knowledge Bases. Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence.</p>
			</li>
			<li id="unger2012template" about="unger2012template-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="unger2012template-work" role="doc-biblioentry">
				<p about="unger2012template-expression" property="dcterms:bibliographicCitation">Unger C., Buhmann L., Lehmann J., Ngonga Ngomo A., Gerber D., Cimiano P. (2012). Template-based Question Answering over RDF Data. Proceedings of the 21st International Conference on World Wide Web.</p>
			</li>
			<li id="SINA_WebSemantic" about="SINA_WebSemantic-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="SINA_WebSemantic-work" role="doc-biblioentry">
				<p about="SINA_WebSemantic-expression" property="dcterms:bibliographicCitation">Shekarpour S., Marx E., Ngomo A., Auer S. (2015). {SINA}: Semantic Interpretation of User Queries for {Q}uestion {Answering} On Interlinked Data. Journal of Web Semantics.</p>
			</li>
			<li id="lukovnikov2017www" about="lukovnikov2017www-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="lukovnikov2017www-work" role="doc-biblioentry">
				<p about="lukovnikov2017www-expression" property="dcterms:bibliographicCitation">Lukovnikov D., Fischer A., Auer S., Lehmann J. (2017). Neural Network-based Question Answering over Knowledge Graphs on Word and Character Level. Proceedings of the 26th international conference on World Wide Web.</p>
			</li>
			<li id="isem2013daiber" about="isem2013daiber-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="isem2013daiber-work" role="doc-biblioentry">
				<p about="isem2013daiber-expression" property="dcterms:bibliographicCitation">Joachim Daiber, Max Jakob, Chris Hokamp, Pablo N. Mendes (2013). Improving Efficiency and Accuracy in Multilingual Entity Extraction. UNKNOWN.</p>
			</li>
			<!-- <li id="koehn2009statistical" about="koehn2009statistical-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="koehn2009statistical-work" role="doc-biblioentry">
				<p about="koehn2009statistical-expression" property="dcterms:bibliographicCitation">Koehn P. (2009). Statistical machine translation. UNKNOWN.</p>
			</li> -->
			<!-- <li id="mikolov2010recurrent" about="mikolov2010recurrent-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="mikolov2010recurrent-work" role="doc-biblioentry">
				<p about="mikolov2010recurrent-expression" property="dcterms:bibliographicCitation">Mikolov T., Karafiat M., Burget L., Cernocky J., Khudanpur S. (2010). Recurrent neural network based language model. Interspeech.</p>
			</li> -->
			<li id="kalchbrenner2013recurrent" about="kalchbrenner2013recurrent-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="kalchbrenner2013recurrent-work" role="doc-biblioentry">
				<p about="kalchbrenner2013recurrent-expression" property="dcterms:bibliographicCitation">Kalchbrenner N., Blunsom P. (2013). Recurrent Continuous Translation Models. EMNLP.</p>
			</li>
			<!-- <li id="bahdanau2014neural" about="bahdanau2014neural-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="bahdanau2014neural-work" role="doc-biblioentry">
				<p about="bahdanau2014neural-expression" property="dcterms:bibliographicCitation">Bahdanau D., Cho K., Bengio Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</p>
			</li> -->
			<li id="wu2016google" about="wu2016google-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="wu2016google-work" role="doc-biblioentry">
				<p about="wu2016google-expression" property="dcterms:bibliographicCitation">Wu Y., Schuster M., Chen Z., Le Q., Norouzi M., Macherey W., Krikun M., Cao Y., Gao Q., Macherey K., others (2016). Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.</p>
			</li>
			<!-- <li id="gehring2017convolutional" about="gehring2017convolutional-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="gehring2017convolutional-work" role="doc-biblioentry">
				<p about="gehring2017convolutional-expression" property="dcterms:bibliographicCitation">Gehring J., Auli M., Grangier D., Yarats D., Dauphin Y. (2017). Convolutional Sequence to Sequence Learning. arXiv preprint arXiv:1705.03122.</p>
			</li> -->
			<li id="abadi2016tensorflow" about="abadi2016tensorflow-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="abadi2016tensorflow-work" role="doc-biblioentry">
				<p about="abadi2016tensorflow-expression" property="dcterms:bibliographicCitation">Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G., Davis A., Dean J., Devin M., others (2016). Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467.</p>
			</li>
			<li id="papineni2002bleu" about="papineni2002bleu-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="papineni2002bleu-work" role="doc-biblioentry">
				<p about="papineni2002bleu-expression" property="dcterms:bibliographicCitation">Papineni K., Roukos S., Ward T., Zhu W. (2002). BLEU: a method for automatic evaluation of machine translation. Proceedings of the 40th annual meeting on association for computational linguistics.</p>
			</li>
			<li id="yih2015" about="yih2015-expression" typeof="fabio:ResearchPaper" property="frbr:realizationOf" resource="yih2015-work" role="doc-biblioentry">
				<p about="yih2015-expression" property="dcterms:bibliographicCitation">Yih W., Chang M., He X., Gao J. (2015). Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base. 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015).</p>
			</li>
            </ul>
        </section>

        <!-- Footnotes -->
        <section role="doc-endnotes">
            <section id="fn1" role="doc-endnote">
                <p><a href="http://www.w3.org/TR/rdf-sparql-query/">http://www.w3.org/TR/rdf-sparql-query/</a></p>
            </section>
            <section id="fn2" role="doc-endnote">
                <p>For a definition of graph patterns, please visit <a href="https://www.w3.org/TR/rdf-sparql-query/">https://www.w3.org/TR/rdf-sparql-query/</a>.</p>
            </section>
            <section id="fn3" role="doc-endnote">
                <p>Source code and datasets available at <a href="http://w3id.org/neural-sparql-machines/code/">http://w3id.org/neural-sparql-machines/code/</a>.</p>
            </section>
            <section id="fn4" role="doc-endnote">
                <p><a href="https://lucene.apache.org">https://lucene.apache.org</a></p>
            </section>
        </section>
    </body>
</html>
